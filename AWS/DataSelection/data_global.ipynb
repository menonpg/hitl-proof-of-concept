{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e466b001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported DATASETS module\n",
      "Available classes: ['CONSOLIDATE', 'COPY', 'CREATE', 'EXTRACT', 'MERGE', 'MODALITY', 'MOVE', 'SPLIT', 'STATE', 'TYPE']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Add current directory to path to import data_global\n",
    "# In Jupyter notebooks, use os.getcwd() instead of __file__\n",
    "notebook_dir = Path(os.getcwd()) if '__file__' not in globals() else Path(__file__).parent\n",
    "sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "# Import the DATASETS class\n",
    "from data_global import DATASETS as DS\n",
    "\n",
    "print(\"‚úÖ Successfully imported DATASETS module\")\n",
    "print(\"Available classes:\", [attr for attr in dir(DS) if not attr.startswith('_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bbe4c",
   "metadata": {},
   "source": [
    "# DETR Dataset Upload to S3 - HITL Demo Pipeline\n",
    "\n",
    "**Date:** December 26, 2025  \n",
    "**Purpose:** Upload DETR utility detection datasets to S3 for Human-in-the-Loop (HITL) workflow  \n",
    "**S3 Endpoint:** https://s3.ohl-inspection.com  \n",
    "**Target Bucket:** siemens-hitl-demo\n",
    "\n",
    "## Proposed S3 Structure\n",
    "```\n",
    "s3://siemens-hitl-demo/\n",
    "‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ insulators/          # Original insulator dataset\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ crossarms/           # Original crossarm dataset  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ utility-poles/       # Original utility pole dataset\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ merged/              # Combined DETR dataset (923 images)\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ valid/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ test/\n",
    "‚îî‚îÄ‚îÄ consolidated/            # Single folder with all data for HITL\n",
    "    ‚îú‚îÄ‚îÄ images/\n",
    "    ‚îî‚îÄ‚îÄ _annotations.coco.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf4a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuration:\n",
      "   DETR Merged Dataset: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\n",
      "   Workspace: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\n",
      "   Temp Consolidate: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\temp_consolidated\n",
      "   S3 Bucket: prahlad-siemens-hitl-demo\n",
      "   S3 Endpoint: https://s3.ohl-inspection.com\n",
      "\n",
      "‚úÖ DETR merged dataset found\n",
      "   train: 1 files\n",
      "   valid: 1 files\n",
      "   test: 1 files\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "AWS_CLI = r\"C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli\"\n",
    "S3_ENDPOINT = \"https://s3.ohl-inspection.com\"\n",
    "BUCKET_NAME = \"prahlad-siemens-hitl-demo\"  # Updated to unique bucket name\n",
    "\n",
    "# Local paths\n",
    "DETR_MERGED_PATH = Path(r\"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\")\n",
    "WORKSPACE_PATH = Path(r\"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\")\n",
    "TEMP_CONSOLIDATE_PATH = WORKSPACE_PATH / \"temp_consolidated\"\n",
    "\n",
    "print(\"üìÅ Configuration:\")\n",
    "print(f\"   DETR Merged Dataset: {DETR_MERGED_PATH}\")\n",
    "print(f\"   Workspace: {WORKSPACE_PATH}\")\n",
    "print(f\"   Temp Consolidate: {TEMP_CONSOLIDATE_PATH}\")\n",
    "print(f\"   S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"   S3 Endpoint: {S3_ENDPOINT}\")\n",
    "\n",
    "# Verify local dataset exists\n",
    "if DETR_MERGED_PATH.exists():\n",
    "    print(f\"\\n‚úÖ DETR merged dataset found\")\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    for split in splits:\n",
    "        split_path = DETR_MERGED_PATH / split\n",
    "        if split_path.exists():\n",
    "            files = list(split_path.glob('*'))\n",
    "            print(f\"   {split}: {len(files)} files\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: DETR merged dataset not found at {DETR_MERGED_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a92fe",
   "metadata": {},
   "source": [
    "## Step 1: Check S3 Connection and Create Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f40b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Checking existing S3 buckets...\n",
      "============================================================\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 ls --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "2025-12-17 10:23:10 prahlad-test-bucket\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  Bucket 'siemens-hitl-demo' does not exist\n",
      "Creating bucket 'siemens-hitl-demo'...\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 mb s3://siemens-hitl-demo --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "2025-12-17 10:23:10 prahlad-test-bucket\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  Bucket 'siemens-hitl-demo' does not exist\n",
      "Creating bucket 'siemens-hitl-demo'...\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 mb s3://siemens-hitl-demo --endpoint-url https://s3.ohl-inspection.com\n",
      "‚ùå Error:\n",
      "make_bucket failed: s3://siemens-hitl-demo An error occurred (InvalidLocationConstraint) when calling the CreateBucket operation: The eu-central-1 location constraint is not valid.\n",
      "\n",
      "‚ùå Failed to create bucket. Check permissions and endpoint.\n",
      "‚ùå Error:\n",
      "make_bucket failed: s3://siemens-hitl-demo An error occurred (InvalidLocationConstraint) when calling the CreateBucket operation: The eu-central-1 location constraint is not valid.\n",
      "\n",
      "‚ùå Failed to create bucket. Check permissions and endpoint.\n"
     ]
    }
   ],
   "source": [
    "def run_aws_command(command, capture_output=True):\n",
    "    \"\"\"Helper function to run AWS CLI commands\"\"\"\n",
    "    full_cmd = f'{AWS_CLI} {command} --endpoint-url {S3_ENDPOINT}'\n",
    "    print(f\"üîß Running: {full_cmd}\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        full_cmd,\n",
    "        shell=True,\n",
    "        capture_output=capture_output,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        if capture_output and result.stdout:\n",
    "            print(f\"‚úÖ Success:\\n{result.stdout}\")\n",
    "        return True, result.stdout\n",
    "    else:\n",
    "        print(f\"‚ùå Error:\\n{result.stderr}\")\n",
    "        return False, result.stderr\n",
    "\n",
    "# Check current buckets\n",
    "print(\"=\" * 60)\n",
    "print(\"Checking existing S3 buckets...\")\n",
    "print(\"=\" * 60)\n",
    "success, output = run_aws_command(\"s3 ls\")\n",
    "\n",
    "# Check if our target bucket exists\n",
    "if BUCKET_NAME in output:\n",
    "    print(f\"\\n‚úÖ Bucket '{BUCKET_NAME}' already exists\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Bucket '{BUCKET_NAME}' does not exist\")\n",
    "    print(f\"Creating bucket '{BUCKET_NAME}'...\")\n",
    "    \n",
    "    # Create bucket without location constraint (Ceph doesn't use AWS regions)\n",
    "    success, output = run_aws_command(f\"s3 mb s3://{BUCKET_NAME}\")\n",
    "    \n",
    "    if success:\n",
    "        print(f\"‚úÖ Successfully created bucket '{BUCKET_NAME}'\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to create bucket. Check permissions and endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a088ce9",
   "metadata": {},
   "source": [
    "## Step 2: Consolidate DETR Dataset using CONSOLIDATE.json_COCO_V0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e32a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Consolidating DETR dataset using DS.CONSOLIDATE.json_COCO_V0()\n",
      "============================================================\n",
      "Found 3 COCO annotation files to consolidate\n",
      "Processing c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\\_annotations.coco.json\n",
      "Processing c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\\_annotations.coco.json\n",
      "Processing c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\\_annotations.coco.json\n",
      "Copying images to consolidated directory...\n",
      "\n",
      "‚úÖ Consolidation complete!\n",
      "{\n",
      "  \"input_directory\": \"c:\\\\Users\\\\Z0057P7S\\\\OneDrive - Siemens Energy\\\\Documents\\\\MenonSiemens\\\\DETR\\\\utility-inventory-detr-main\\\\datasets\\\\processed\\\\merged\",\n",
      "  \"output_directory\": \"c:\\\\Users\\\\Z0057P7S\\\\OneDrive - Siemens Energy\\\\Documents\\\\MenonSiemens\\\\temp_consolidated\\\\detr_utility_merged\",\n",
      "  \"consolidated_directory\": \"c:\\\\Users\\\\Z0057P7S\\\\OneDrive - Siemens Energy\\\\Documents\\\\MenonSiemens\\\\temp_consolidated\\\\detr_utility_merged\\\\consolidated\",\n",
      "  \"num_images\": 923,\n",
      "  \"num_annotations\": 1024,\n",
      "  \"num_categories\": 3,\n",
      "  \"num_files_processed\": 3,\n",
      "  \"images_copied\": 0,\n",
      "  \"image_filename_map\": {}\n",
      "}\n",
      "\n",
      "üìä Consolidated Dataset Summary:\n",
      "   Location: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\temp_consolidated\\detr_utility_merged\\consolidated\n",
      "   Total files: 1\n",
      "   Image files: 0\n",
      "   JSON files: 1\n",
      "   Images in JSON: 923\n",
      "   Annotations: 1024\n",
      "   Categories: 3\n"
     ]
    }
   ],
   "source": [
    "# Clean up any previous consolidation\n",
    "if TEMP_CONSOLIDATE_PATH.exists():\n",
    "    print(f\"üßπ Cleaning up previous consolidation at {TEMP_CONSOLIDATE_PATH}\")\n",
    "    import shutil\n",
    "    shutil.rmtree(TEMP_CONSOLIDATE_PATH)\n",
    "\n",
    "# Create consolidated dataset from train/valid/test splits\n",
    "print(\"=\" * 60)\n",
    "print(\"Consolidating DETR dataset using DS.CONSOLIDATE.json_COCO_V0()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = DS.CONSOLIDATE.json_COCO_V0(\n",
    "    input_dir=str(DETR_MERGED_PATH),\n",
    "    output_dir=str(WORKSPACE_PATH / \"temp_consolidated\"),\n",
    "    dataset_name=\"detr_utility_merged\",\n",
    "    dataset_description=\"DETR Utility Inventory Dataset - Insulators, Crossarms, Utility Poles\",\n",
    "    dataset_version=\"1.0\",\n",
    "    dataset_year=2025,\n",
    "    dataset_contributer=\"Prahlad Menon, Vijay Kovuru, Bhargav Bompalli, Erick Allage\",\n",
    "    dataset_url=\"https://s3.ohl-inspection.com/siemens-hitl-demo\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Consolidation complete!\")\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "# Verify consolidated output\n",
    "consolidated_dir = Path(result['consolidated_directory'])\n",
    "if consolidated_dir.exists():\n",
    "    files = list(consolidated_dir.glob('*'))\n",
    "    images = [f for f in files if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "    json_files = list(consolidated_dir.glob('*.json'))\n",
    "    \n",
    "    print(f\"\\nüìä Consolidated Dataset Summary:\")\n",
    "    print(f\"   Location: {consolidated_dir}\")\n",
    "    print(f\"   Total files: {len(files)}\")\n",
    "    print(f\"   Image files: {len(images)}\")\n",
    "    print(f\"   JSON files: {len(json_files)}\")\n",
    "    print(f\"   Images in JSON: {result['num_images']}\")\n",
    "    print(f\"   Annotations: {result['num_annotations']}\")\n",
    "    print(f\"   Categories: {result['num_categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58f77e",
   "metadata": {},
   "source": [
    "## Step 3: Upload Split Datasets to S3 (raw/merged/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9365f292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Uploading split datasets (train/valid/test) to S3\n",
      "============================================================\n",
      "\n",
      "üì§ Uploading train split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/train/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\" s3://prahlad-siemens-hitl-demo/raw/merged/train/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 480.6 KiB/480.6 KiB (3.2 MiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/train/_annotations.coco.json\n",
      "\n",
      "‚úÖ train uploaded successfully\n",
      "\n",
      "üì§ Uploading valid split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/valid/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\" s3://prahlad-siemens-hitl-demo/raw/merged/valid/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 480.6 KiB/480.6 KiB (3.2 MiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/train/_annotations.coco.json\n",
      "\n",
      "‚úÖ train uploaded successfully\n",
      "\n",
      "üì§ Uploading valid split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/valid/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\" s3://prahlad-siemens-hitl-demo/raw/merged/valid/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 109.0 KiB/109.0 KiB (93.2 KiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/valid/_annotations.coco.json\n",
      "\n",
      "‚úÖ valid uploaded successfully\n",
      "\n",
      "üì§ Uploading test split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/test/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\" s3://prahlad-siemens-hitl-demo/raw/merged/test/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 109.0 KiB/109.0 KiB (93.2 KiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/valid/_annotations.coco.json\n",
      "\n",
      "‚úÖ valid uploaded successfully\n",
      "\n",
      "üì§ Uploading test split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/test/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\" s3://prahlad-siemens-hitl-demo/raw/merged/test/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 53.2 KiB/53.2 KiB (55.8 KiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/test/_annotations.coco.json\n",
      "\n",
      "‚úÖ test uploaded successfully\n",
      "\n",
      "‚úÖ All splits uploaded to s3://{BUCKET_NAME}/raw/merged/\n",
      "‚úÖ Success:\n",
      "Completed 53.2 KiB/53.2 KiB (55.8 KiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/test/_annotations.coco.json\n",
      "\n",
      "‚úÖ test uploaded successfully\n",
      "\n",
      "‚úÖ All splits uploaded to s3://{BUCKET_NAME}/raw/merged/\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Uploading split datasets (train/valid/test) to S3\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Upload each split to raw/merged/ in S3\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "for split in splits:\n",
    "    split_path = DETR_MERGED_PATH / split\n",
    "    \n",
    "    if not split_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Skipping {split} - directory not found\")\n",
    "        continue\n",
    "    \n",
    "    s3_prefix = f\"s3://{BUCKET_NAME}/raw/merged/{split}/\"\n",
    "    \n",
    "    print(f\"\\nüì§ Uploading {split} split...\")\n",
    "    print(f\"   Source: {split_path}\")\n",
    "    print(f\"   Destination: {s3_prefix}\")\n",
    "    \n",
    "    # Use AWS CLI sync command with STANDARD storage class\n",
    "    cmd = f's3 sync \"{split_path}\" {s3_prefix} --storage-class STANDARD'\n",
    "    success, output = run_aws_command(cmd, capture_output=True)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"‚úÖ {split} uploaded successfully\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to upload {split}\")\n",
    "\n",
    "print(\"\\n‚úÖ All splits uploaded to s3://{BUCKET_NAME}/raw/merged/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed9985",
   "metadata": {},
   "source": [
    "## Step 4: Upload Consolidated Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd75172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Uploading consolidated dataset to S3\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Upload consolidated dataset to S3\n",
    "consolidated_path = Path(result['consolidated_directory'])\n",
    "s3_consolidated_prefix = f\"s3://{BUCKET_NAME}/consolidated/\"\n",
    "\n",
    "print(f\"\\nüì§ Uploading consolidated dataset...\")\n",
    "print(f\"   Source: {consolidated_path}\")\n",
    "print(f\"   Destination: {s3_consolidated_prefix}\")\n",
    "\n",
    "# Sync consolidated folder to S3 with STANDARD storage class\n",
    "cmd = f's3 sync \"{consolidated_path}\" {s3_consolidated_prefix} --storage-class STANDARD'\n",
    "success, output = run_aws_command(cmd, capture_output=True)\n",
    "\n",
    "if success:\n",
    "    print(f\"‚úÖ Consolidated dataset uploaded successfully\")\n",
    "    print(f\"\\nüìä Upload Summary:\")\n",
    "    print(f\"   Bucket: {BUCKET_NAME}\")\n",
    "    print(f\"   Endpoint: {S3_ENDPOINT}\")\n",
    "    print(f\"   Consolidated path: s3://{BUCKET_NAME}/consolidated/\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to upload consolidated dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
