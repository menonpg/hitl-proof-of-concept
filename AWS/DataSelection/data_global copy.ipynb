{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e466b001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported DATASETS module\n",
      "Available classes: ['CONSOLIDATE', 'COPY', 'CREATE', 'EXTRACT', 'MERGE', 'MODALITY', 'MOVE', 'SPLIT', 'STATE', 'TYPE']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Add current directory to path to import data_global\n",
    "# In Jupyter notebooks, use os.getcwd() instead of __file__\n",
    "notebook_dir = Path(os.getcwd()) if '__file__' not in globals() else Path(__file__).parent\n",
    "sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "# Import the DATASETS class\n",
    "from data_global import DATASETS as DS\n",
    "\n",
    "print(\"‚úÖ Successfully imported DATASETS module\")\n",
    "print(\"Available classes:\", [attr for attr in dir(DS) if not attr.startswith('_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bbe4c",
   "metadata": {},
   "source": [
    "# DETR Dataset Upload to S3 - HITL Demo Pipeline\n",
    "\n",
    "**Date:** December 26, 2025  \n",
    "**Purpose:** Upload DETR utility detection datasets to S3 for Human-in-the-Loop (HITL) workflow  \n",
    "**S3 Endpoint:** https://s3.ohl-inspection.com  \n",
    "**Target Bucket:** siemens-hitl-demo\n",
    "\n",
    "## Proposed S3 Structure\n",
    "```\n",
    "s3://siemens-hitl-demo/\n",
    "‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ insulators/          # Original insulator dataset\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ crossarms/           # Original crossarm dataset  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ utility-poles/       # Original utility pole dataset\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ merged/              # Combined DETR dataset (923 images)\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ valid/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ test/\n",
    "‚îî‚îÄ‚îÄ consolidated/            # Single folder with all data for HITL\n",
    "    ‚îú‚îÄ‚îÄ images/\n",
    "    ‚îî‚îÄ‚îÄ _annotations.coco.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf4a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuration:\n",
      "   DETR Merged Dataset: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\n",
      "   Workspace: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\n",
      "   Temp Consolidate: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\temp_consolidated\n",
      "   S3 Bucket: prahlad-siemens-hitl-demo\n",
      "   S3 Endpoint: https://s3.ohl-inspection.com\n",
      "\n",
      "‚úÖ DETR merged dataset found\n",
      "   train: 1 files\n",
      "   valid: 1 files\n",
      "   test: 1 files\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "AWS_CLI = r\"C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli\"\n",
    "S3_ENDPOINT = \"https://s3.ohl-inspection.com\"\n",
    "BUCKET_NAME = \"prahlad-siemens-hitl-demo\"  # Updated to unique bucket name\n",
    "\n",
    "# Local paths\n",
    "DETR_MERGED_PATH = Path(r\"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\")\n",
    "WORKSPACE_PATH = Path(r\"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\")\n",
    "TEMP_CONSOLIDATE_PATH = WORKSPACE_PATH / \"temp_consolidated\"\n",
    "\n",
    "print(\"üìÅ Configuration:\")\n",
    "print(f\"   DETR Merged Dataset: {DETR_MERGED_PATH}\")\n",
    "print(f\"   Workspace: {WORKSPACE_PATH}\")\n",
    "print(f\"   Temp Consolidate: {TEMP_CONSOLIDATE_PATH}\")\n",
    "print(f\"   S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"   S3 Endpoint: {S3_ENDPOINT}\")\n",
    "\n",
    "# Verify local dataset exists\n",
    "if DETR_MERGED_PATH.exists():\n",
    "    print(f\"\\n‚úÖ DETR merged dataset found\")\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    for split in splits:\n",
    "        split_path = DETR_MERGED_PATH / split\n",
    "        if split_path.exists():\n",
    "            files = list(split_path.glob('*'))\n",
    "            print(f\"   {split}: {len(files)} files\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ERROR: DETR merged dataset not found at {DETR_MERGED_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a92fe",
   "metadata": {},
   "source": [
    "## Step 1: Check S3 Connection and Create Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f40b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Checking existing S3 buckets...\n",
      "============================================================\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 ls --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "2025-12-17 10:23:10 prahlad-test-bucket\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  Bucket 'siemens-hitl-demo' does not exist\n",
      "Creating bucket 'siemens-hitl-demo'...\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 mb s3://siemens-hitl-demo --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "2025-12-17 10:23:10 prahlad-test-bucket\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  Bucket 'siemens-hitl-demo' does not exist\n",
      "Creating bucket 'siemens-hitl-demo'...\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 mb s3://siemens-hitl-demo --endpoint-url https://s3.ohl-inspection.com\n",
      "‚ùå Error:\n",
      "make_bucket failed: s3://siemens-hitl-demo An error occurred (InvalidLocationConstraint) when calling the CreateBucket operation: The eu-central-1 location constraint is not valid.\n",
      "\n",
      "‚ùå Failed to create bucket. Check permissions and endpoint.\n",
      "‚ùå Error:\n",
      "make_bucket failed: s3://siemens-hitl-demo An error occurred (InvalidLocationConstraint) when calling the CreateBucket operation: The eu-central-1 location constraint is not valid.\n",
      "\n",
      "‚ùå Failed to create bucket. Check permissions and endpoint.\n"
     ]
    }
   ],
   "source": [
    "def run_aws_command(command, capture_output=True):\n",
    "    \"\"\"Helper function to run AWS CLI commands\"\"\"\n",
    "    full_cmd = f'{AWS_CLI} {command} --endpoint-url {S3_ENDPOINT}'\n",
    "    print(f\"üîß Running: {full_cmd}\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        full_cmd,\n",
    "        shell=True,\n",
    "        capture_output=capture_output,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        if capture_output and result.stdout:\n",
    "            print(f\"‚úÖ Success:\\n{result.stdout}\")\n",
    "        return True, result.stdout\n",
    "    else:\n",
    "        print(f\"‚ùå Error:\\n{result.stderr}\")\n",
    "        return False, result.stderr\n",
    "\n",
    "# Check current buckets\n",
    "print(\"=\" * 60)\n",
    "print(\"Checking existing S3 buckets...\")\n",
    "print(\"=\" * 60)\n",
    "success, output = run_aws_command(\"s3 ls\")\n",
    "\n",
    "# Check if our target bucket exists\n",
    "if BUCKET_NAME in output:\n",
    "    print(f\"\\n‚úÖ Bucket '{BUCKET_NAME}' already exists\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Bucket '{BUCKET_NAME}' does not exist\")\n",
    "    print(f\"Creating bucket '{BUCKET_NAME}'...\")\n",
    "    \n",
    "    # Create bucket without location constraint (Ceph doesn't use AWS regions)\n",
    "    success, output = run_aws_command(f\"s3 mb s3://{BUCKET_NAME}\")\n",
    "    \n",
    "    if success:\n",
    "        print(f\"‚úÖ Successfully created bucket '{BUCKET_NAME}'\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to create bucket. Check permissions and endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a088ce9",
   "metadata": {},
   "source": [
    "## Step 2: Consolidate DETR Dataset using CONSOLIDATE.json_COCO_V0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e32a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Consolidating DETR dataset using DS.CONSOLIDATE.json_COCO_V0()\n",
      "============================================================\n",
      "Found 3 COCO annotation files to consolidate\n",
      "Processing c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\\_annotations.coco.json\n",
      "Processing c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\\_annotations.coco.json\n",
      "Processing c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\\_annotations.coco.json\n",
      "Copying images to consolidated directory...\n",
      "\n",
      "‚úÖ Consolidation complete!\n",
      "{\n",
      "  \"input_directory\": \"c:\\\\Users\\\\Z0057P7S\\\\OneDrive - Siemens Energy\\\\Documents\\\\MenonSiemens\\\\DETR\\\\utility-inventory-detr-main\\\\datasets\\\\processed\\\\merged\",\n",
      "  \"output_directory\": \"c:\\\\Users\\\\Z0057P7S\\\\OneDrive - Siemens Energy\\\\Documents\\\\MenonSiemens\\\\temp_consolidated\\\\detr_utility_merged\",\n",
      "  \"consolidated_directory\": \"c:\\\\Users\\\\Z0057P7S\\\\OneDrive - Siemens Energy\\\\Documents\\\\MenonSiemens\\\\temp_consolidated\\\\detr_utility_merged\\\\consolidated\",\n",
      "  \"num_images\": 923,\n",
      "  \"num_annotations\": 1024,\n",
      "  \"num_categories\": 3,\n",
      "  \"num_files_processed\": 3,\n",
      "  \"images_copied\": 0,\n",
      "  \"image_filename_map\": {}\n",
      "}\n",
      "\n",
      "üìä Consolidated Dataset Summary:\n",
      "   Location: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\temp_consolidated\\detr_utility_merged\\consolidated\n",
      "   Total files: 1\n",
      "   Image files: 0\n",
      "   JSON files: 1\n",
      "   Images in JSON: 923\n",
      "   Annotations: 1024\n",
      "   Categories: 3\n"
     ]
    }
   ],
   "source": [
    "# Clean up any previous consolidation\n",
    "if TEMP_CONSOLIDATE_PATH.exists():\n",
    "    print(f\"üßπ Cleaning up previous consolidation at {TEMP_CONSOLIDATE_PATH}\")\n",
    "    import shutil\n",
    "    shutil.rmtree(TEMP_CONSOLIDATE_PATH)\n",
    "\n",
    "# Create consolidated dataset from train/valid/test splits\n",
    "print(\"=\" * 60)\n",
    "print(\"Consolidating DETR dataset using DS.CONSOLIDATE.json_COCO_V0()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = DS.CONSOLIDATE.json_COCO_V0(\n",
    "    input_dir=str(DETR_MERGED_PATH),\n",
    "    output_dir=str(WORKSPACE_PATH / \"temp_consolidated\"),\n",
    "    dataset_name=\"detr_utility_merged\",\n",
    "    dataset_description=\"DETR Utility Inventory Dataset - Insulators, Crossarms, Utility Poles\",\n",
    "    dataset_version=\"1.0\",\n",
    "    dataset_year=2025,\n",
    "    dataset_contributer=\"Prahlad Menon, Vijay Kovuru, Bhargav Bompalli, Erick Allage\",\n",
    "    dataset_url=\"https://s3.ohl-inspection.com/siemens-hitl-demo\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Consolidation complete!\")\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "# Verify consolidated output\n",
    "consolidated_dir = Path(result['consolidated_directory'])\n",
    "if consolidated_dir.exists():\n",
    "    files = list(consolidated_dir.glob('*'))\n",
    "    images = [f for f in files if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "    json_files = list(consolidated_dir.glob('*.json'))\n",
    "    \n",
    "    print(f\"\\nüìä Consolidated Dataset Summary:\")\n",
    "    print(f\"   Location: {consolidated_dir}\")\n",
    "    print(f\"   Total files: {len(files)}\")\n",
    "    print(f\"   Image files: {len(images)}\")\n",
    "    print(f\"   JSON files: {len(json_files)}\")\n",
    "    print(f\"   Images in JSON: {result['num_images']}\")\n",
    "    print(f\"   Annotations: {result['num_annotations']}\")\n",
    "    print(f\"   Categories: {result['num_categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58f77e",
   "metadata": {},
   "source": [
    "## Step 3: Upload Split Datasets to S3 (raw/merged/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9365f292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Uploading split datasets (train/valid/test) to S3\n",
      "============================================================\n",
      "\n",
      "üì§ Uploading train split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/train/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\" s3://prahlad-siemens-hitl-demo/raw/merged/train/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 480.6 KiB/480.6 KiB (3.2 MiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/train/_annotations.coco.json\n",
      "\n",
      "‚úÖ train uploaded successfully\n",
      "\n",
      "üì§ Uploading valid split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/valid/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\" s3://prahlad-siemens-hitl-demo/raw/merged/valid/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 480.6 KiB/480.6 KiB (3.2 MiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\train\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/train/_annotations.coco.json\n",
      "\n",
      "‚úÖ train uploaded successfully\n",
      "\n",
      "üì§ Uploading valid split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/valid/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\" s3://prahlad-siemens-hitl-demo/raw/merged/valid/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 109.0 KiB/109.0 KiB (93.2 KiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/valid/_annotations.coco.json\n",
      "\n",
      "‚úÖ valid uploaded successfully\n",
      "\n",
      "üì§ Uploading test split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/test/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\" s3://prahlad-siemens-hitl-demo/raw/merged/test/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 109.0 KiB/109.0 KiB (93.2 KiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\valid\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/valid/_annotations.coco.json\n",
      "\n",
      "‚úÖ valid uploaded successfully\n",
      "\n",
      "üì§ Uploading test split...\n",
      "   Source: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\n",
      "   Destination: s3://prahlad-siemens-hitl-demo/raw/merged/test/\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 sync \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\" s3://prahlad-siemens-hitl-demo/raw/merged/test/ --storage-class STANDARD --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "Completed 53.2 KiB/53.2 KiB (55.8 KiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/test/_annotations.coco.json\n",
      "\n",
      "‚úÖ test uploaded successfully\n",
      "\n",
      "‚úÖ All splits uploaded to s3://{BUCKET_NAME}/raw/merged/\n",
      "‚úÖ Success:\n",
      "Completed 53.2 KiB/53.2 KiB (55.8 KiB/s) with 1 file(s) remaining\n",
      "upload: ..\\..\\DETR\\utility-inventory-detr-main\\datasets\\processed\\merged\\test\\_annotations.coco.json to s3://prahlad-siemens-hitl-demo/raw/merged/test/_annotations.coco.json\n",
      "\n",
      "‚úÖ test uploaded successfully\n",
      "\n",
      "‚úÖ All splits uploaded to s3://{BUCKET_NAME}/raw/merged/\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Uploading split datasets (train/valid/test) to S3\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Upload each split to raw/merged/ in S3\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "for split in splits:\n",
    "    split_path = DETR_MERGED_PATH / split\n",
    "    \n",
    "    if not split_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Skipping {split} - directory not found\")\n",
    "        continue\n",
    "    \n",
    "    s3_prefix = f\"s3://{BUCKET_NAME}/raw/merged/{split}/\"\n",
    "    \n",
    "    print(f\"\\nüì§ Uploading {split} split...\")\n",
    "    print(f\"   Source: {split_path}\")\n",
    "    print(f\"   Destination: {s3_prefix}\")\n",
    "    \n",
    "    # Use AWS CLI sync command with STANDARD storage class\n",
    "    cmd = f's3 sync \"{split_path}\" {s3_prefix} --storage-class STANDARD'\n",
    "    success, output = run_aws_command(cmd, capture_output=True)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"‚úÖ {split} uploaded successfully\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to upload {split}\")\n",
    "\n",
    "print(\"\\n‚úÖ All splits uploaded to s3://{BUCKET_NAME}/raw/merged/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed9985",
   "metadata": {},
   "source": [
    "## Step 4: Upload Consolidated Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd75172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Uploading consolidated dataset to S3\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Upload consolidated dataset to S3\n",
    "consolidated_path = Path(result['consolidated_directory'])\n",
    "s3_consolidated_prefix = f\"s3://{BUCKET_NAME}/consolidated/\"\n",
    "\n",
    "print(f\"\\nüì§ Uploading consolidated dataset...\")\n",
    "print(f\"   Source: {consolidated_path}\")\n",
    "print(f\"   Destination: {s3_consolidated_prefix}\")\n",
    "\n",
    "# Sync consolidated folder to S3 with STANDARD storage class\n",
    "cmd = f's3 sync \"{consolidated_path}\" {s3_consolidated_prefix} --storage-class STANDARD'\n",
    "success, output = run_aws_command(cmd, capture_output=True)\n",
    "\n",
    "if success:\n",
    "    print(f\"‚úÖ Consolidated dataset uploaded successfully\")\n",
    "    print(f\"\\nüìä Upload Summary:\")\n",
    "    print(f\"   Bucket: {BUCKET_NAME}\")\n",
    "    print(f\"   Endpoint: {S3_ENDPOINT}\")\n",
    "    print(f\"   Consolidated path: s3://{BUCKET_NAME}/consolidated/\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to upload consolidated dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc33c0",
   "metadata": {},
   "source": [
    "## Step 5: Verify S3 Upload and List Bucket Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b93b9932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Verifying S3 Bucket Structure\n",
      "============================================================\n",
      "\n",
      "üìÇ Bucket structure:\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 ls s3://prahlad-siemens-hitl-demo/ --recursive --human-readable --summarize --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "2025-12-26 12:18:51   53.2 KiB raw/merged/test/_annotations.coco.json\n",
      "2025-12-26 12:18:42  480.6 KiB raw/merged/train/_annotations.coco.json\n",
      "2025-12-26 12:18:47  109.0 KiB raw/merged/valid/_annotations.coco.json\n",
      "\n",
      "Total Objects: 3\n",
      "   Total Size: 642.8 KiB\n",
      "\n",
      "\n",
      "üìÇ Raw merged datasets:\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 ls s3://prahlad-siemens-hitl-demo/raw/merged/ --recursive --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "2025-12-26 12:18:51   53.2 KiB raw/merged/test/_annotations.coco.json\n",
      "2025-12-26 12:18:42  480.6 KiB raw/merged/train/_annotations.coco.json\n",
      "2025-12-26 12:18:47  109.0 KiB raw/merged/valid/_annotations.coco.json\n",
      "\n",
      "Total Objects: 3\n",
      "   Total Size: 642.8 KiB\n",
      "\n",
      "\n",
      "üìÇ Raw merged datasets:\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 ls s3://prahlad-siemens-hitl-demo/raw/merged/ --recursive --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "2025-12-26 12:18:51      54479 raw/merged/test/_annotations.coco.json\n",
      "2025-12-26 12:18:42     492143 raw/merged/train/_annotations.coco.json\n",
      "2025-12-26 12:18:47     111587 raw/merged/valid/_annotations.coco.json\n",
      "\n",
      "\n",
      "üìÇ Consolidated dataset:\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 ls s3://prahlad-siemens-hitl-demo/consolidated/ --recursive --endpoint-url https://s3.ohl-inspection.com\n",
      "‚úÖ Success:\n",
      "2025-12-26 12:18:51      54479 raw/merged/test/_annotations.coco.json\n",
      "2025-12-26 12:18:42     492143 raw/merged/train/_annotations.coco.json\n",
      "2025-12-26 12:18:47     111587 raw/merged/valid/_annotations.coco.json\n",
      "\n",
      "\n",
      "üìÇ Consolidated dataset:\n",
      "üîß Running: C:\\Users\\Z0057P7S\\miniconda3\\python.exe -m awscli s3 ls s3://prahlad-siemens-hitl-demo/consolidated/ --recursive --endpoint-url https://s3.ohl-inspection.com\n",
      "‚ùå Error:\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ DETR Dataset Upload Complete!\n",
      "============================================================\n",
      "\n",
      "üéØ Next Steps for HITL Demo:\n",
      "   1. Team can access datasets at: s3://prahlad-siemens-hitl-demo/\n",
      "   2. Use 'consolidated/' folder for single-dataset HITL workflow\n",
      "   3. Use 'raw/merged/' folders for split-based training/validation\n",
      "   4. Implement HITL evaluation scripts to:\n",
      "      - Load model predictions\n",
      "      - Calculate IOU metrics\n",
      "      - Generate correction workflow\n",
      "\n",
      "üìù Dataset Info:\n",
      "   Endpoint: https://s3.ohl-inspection.com\n",
      "   Bucket: prahlad-siemens-hitl-demo\n",
      "   Images: 923\n",
      "   Annotations: 1024\n",
      "   Categories: 3\n",
      "‚ùå Error:\n",
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ DETR Dataset Upload Complete!\n",
      "============================================================\n",
      "\n",
      "üéØ Next Steps for HITL Demo:\n",
      "   1. Team can access datasets at: s3://prahlad-siemens-hitl-demo/\n",
      "   2. Use 'consolidated/' folder for single-dataset HITL workflow\n",
      "   3. Use 'raw/merged/' folders for split-based training/validation\n",
      "   4. Implement HITL evaluation scripts to:\n",
      "      - Load model predictions\n",
      "      - Calculate IOU metrics\n",
      "      - Generate correction workflow\n",
      "\n",
      "üìù Dataset Info:\n",
      "   Endpoint: https://s3.ohl-inspection.com\n",
      "   Bucket: prahlad-siemens-hitl-demo\n",
      "   Images: 923\n",
      "   Annotations: 1024\n",
      "   Categories: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Verifying S3 Bucket Structure\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List bucket contents\n",
    "print(\"\\nüìÇ Bucket structure:\")\n",
    "success, output = run_aws_command(f\"s3 ls s3://{BUCKET_NAME}/ --recursive --human-readable --summarize\")\n",
    "\n",
    "print(\"\\nüìÇ Raw merged datasets:\")\n",
    "success, output = run_aws_command(f\"s3 ls s3://{BUCKET_NAME}/raw/merged/ --recursive\")\n",
    "\n",
    "print(\"\\nüìÇ Consolidated dataset:\")\n",
    "success, output = run_aws_command(f\"s3 ls s3://{BUCKET_NAME}/consolidated/ --recursive\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DETR Dataset Upload Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüéØ Next Steps for HITL Demo:\")\n",
    "print(f\"   1. Team can access datasets at: s3://{BUCKET_NAME}/\")\n",
    "print(f\"   2. Use 'consolidated/' folder for single-dataset HITL workflow\")\n",
    "print(f\"   3. Use 'raw/merged/' folders for split-based training/validation\")\n",
    "print(f\"   4. Implement HITL evaluation scripts to:\")\n",
    "print(f\"      - Load model predictions\")\n",
    "print(f\"      - Calculate IOU metrics\")\n",
    "print(f\"      - Generate correction workflow\")\n",
    "print(f\"\\nüìù Dataset Info:\")\n",
    "print(f\"   Endpoint: {S3_ENDPOINT}\")\n",
    "print(f\"   Bucket: {BUCKET_NAME}\")\n",
    "print(f\"   Images: {result['num_images']}\")\n",
    "print(f\"   Annotations: {result['num_annotations']}\")\n",
    "print(f\"   Categories: {result['num_categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c240a",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Important Finding: Images Not in Repository\n",
    "\n",
    "The DETR repository only contains **annotation JSON files** (metadata), not the actual images.  \n",
    "The images (923 total) were excluded from git due to size constraints.\n",
    "\n",
    "### Current S3 Upload Status:\n",
    "‚úÖ **Uploaded:** Annotation JSONs only (642.8 KiB total)  \n",
    "‚ùå **Missing:** Actual image files (~923 images)\n",
    "\n",
    "### To Complete the Dataset Upload:\n",
    "\n",
    "**Option 1: Download from Roboflow** (Original Source)\n",
    "- Use `DETR/utility-inventory-detr-main/scripts/01_download_datasets.py`\n",
    "- Download the 3 source datasets from Roboflow\n",
    "- Extract using `00_extract_datasets.py`\n",
    "- Then upload images to S3\n",
    "\n",
    "**Option 2: Use Local Training Images** (If Available)\n",
    "- Check if images exist locally from previous training runs\n",
    "- Verify images match the annotation JSON references\n",
    "- Upload directly to S3\n",
    "\n",
    "**Option 3: Request from Team**\n",
    "- Check with Vijay Kovuru who completed the training\n",
    "- May have access to the full dataset with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a87b8",
   "metadata": {},
   "source": [
    "## üì• How to Get the DETR Images - Step by Step Guide\n",
    "\n",
    "### Method 1: Manual Download from Roboflow (Recommended - No API Key Needed)\n",
    "\n",
    "**Step 1: Visit Each Dataset and Download**\n",
    "\n",
    "1. **Insulators Dataset** (599 images)\n",
    "   - URL: https://universe.roboflow.com/sofia-valdivieso-von-teuber/insulators-wo6lb/dataset/3\n",
    "   - Click \"Download Dataset\"\n",
    "   - Select format: **COCO**\n",
    "   - Download ZIP file: `Insulators.v5i.coco.zip`\n",
    "\n",
    "2. **Crossarm Dataset** (207 images)\n",
    "   - URL: https://universe.roboflow.com/project-91iyv/song-crossarm-zqkmo\n",
    "   - Click \"Download Dataset\"\n",
    "   - Select format: **COCO**\n",
    "   - Download ZIP file: `song crossarm.v6i.coco.zip`\n",
    "\n",
    "3. **Utility-pole Dataset** (218 images)\n",
    "   - URL: https://universe.roboflow.com/project-6kpfk/utility-pole-hdbuh\n",
    "   - Click \"Download Dataset\"\n",
    "   - Select format: **COCO**\n",
    "   - Download ZIP file: `utility-pole.v4i.coco.zip`\n",
    "\n",
    "**Step 2: Save ZIP files to workspace**\n",
    "```\n",
    "Save all 3 ZIP files to:\n",
    "c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\\\n",
    "```\n",
    "\n",
    "**Step 3: Extract and Process Datasets**\n",
    "```python\n",
    "# Run in terminal from DETR directory:\n",
    "cd \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\"\n",
    "\n",
    "# Check which files exist\n",
    "python scripts\\01_download_datasets.py\n",
    "\n",
    "# Extract ZIP files\n",
    "python scripts\\00_extract_datasets.py\n",
    "\n",
    "# Clean datasets\n",
    "python scripts\\02_clean_datasets.py\n",
    "\n",
    "# Merge into unified dataset\n",
    "python scripts\\03_merge_datasets.py\n",
    "```\n",
    "\n",
    "**Step 4: Re-run This Notebook**\n",
    "Once images are extracted, re-run the upload cells in this notebook. The images will be:\n",
    "- In `datasets/processed/merged/train/`, `valid/`, `test/`\n",
    "- Automatically uploaded to S3 with the annotation JSONs\n",
    "\n",
    "---\n",
    "\n",
    "### Method 2: Use Roboflow API (If You Have API Key)\n",
    "\n",
    "**Step 1: Get Roboflow API Key**\n",
    "- Sign up at https://roboflow.com/\n",
    "- Go to Settings ‚Üí API Keys\n",
    "- Copy your API key\n",
    "\n",
    "**Step 2: Set Environment Variable**\n",
    "```powershell\n",
    "# In PowerShell:\n",
    "$env:ROBOFLOW_API_KEY = \"your_api_key_here\"\n",
    "```\n",
    "\n",
    "**Step 3: Run Download Script**\n",
    "```python\n",
    "cd \"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\"\n",
    "python scripts\\01_download_datasets.py  # Will use API to download\n",
    "python scripts\\00_extract_datasets.py\n",
    "python scripts\\02_clean_datasets.py\n",
    "python scripts\\03_merge_datasets.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Method 3: Request from Team Member\n",
    "\n",
    "Contact **Vijay Kovuru** (ext) who completed the training. He may have:\n",
    "- The original ZIP files\n",
    "- Already extracted datasets with images\n",
    "- Access to a shared drive with the full dataset\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Result After Download\n",
    "\n",
    "Once complete, you should have:\n",
    "```\n",
    "DETR/utility-inventory-detr-main/datasets/\n",
    "‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ insulators/      (extracted ZIP 1)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ crossarm/        (extracted ZIP 2)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utility-pole/    (extracted ZIP 3)\n",
    "‚îî‚îÄ‚îÄ processed/\n",
    "    ‚îî‚îÄ‚îÄ merged/\n",
    "        ‚îú‚îÄ‚îÄ train/       (713 .jpg images + _annotations.coco.json)\n",
    "        ‚îú‚îÄ‚îÄ valid/       (134 .jpg images + _annotations.coco.json)\n",
    "        ‚îî‚îÄ‚îÄ test/        (76 .jpg images + _annotations.coco.json)\n",
    "```\n",
    "\n",
    "**Total Size:** ~200-500 MB for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e85f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DETR Dataset S3 Upload - Status Report\n",
      "======================================================================\n",
      "\n",
      "‚úÖ COMPLETED:\n",
      "  ‚Ä¢ AWS CLI configured and tested\n",
      "  ‚Ä¢ S3 bucket created: prahlad-siemens-hitl-demo\n",
      "  ‚Ä¢ DATASETS.CONSOLIDATE.json_COCO_V0() function validated\n",
      "  ‚Ä¢ Annotation JSONs uploaded (642.8 KiB)\n",
      "    - train: 713 images metadata\n",
      "    - valid: 134 images metadata\n",
      "    - test: 76 images metadata\n",
      "  ‚Ä¢ Total: 923 images metadata, 1,024 annotations, 3 categories\n",
      "\n",
      "‚ö†Ô∏è  PENDING:\n",
      "  ‚Ä¢ Download 923 image files from Roboflow\n",
      "  ‚Ä¢ Upload images to S3 (est. 200-500 MB)\n",
      "\n",
      "üìä S3 Structure (Current):\n",
      "  s3://prahlad-siemens-hitl-demo/\n",
      "  ‚îî‚îÄ‚îÄ raw/merged/\n",
      "      ‚îú‚îÄ‚îÄ test/_annotations.coco.json\n",
      "      ‚îú‚îÄ‚îÄ train/_annotations.coco.json\n",
      "      ‚îî‚îÄ‚îÄ valid/_annotations.coco.json\n",
      "\n",
      "üéØ Next Steps for Team:\n",
      "  1. Download images from Roboflow using DETR scripts\n",
      "  2. Re-run this notebook to upload images\n",
      "  3. Implement HITL evaluation scripts\n",
      "  4. Build YOLOv11-OBB converter\n",
      "  5. Develop correction UI workflow\n",
      "\n",
      "üìù Resources:\n",
      "  ‚Ä¢ S3 Endpoint: https://s3.ohl-inspection.com\n",
      "  ‚Ä¢ Bucket: prahlad-siemens-hitl-demo\n",
      "  ‚Ä¢ Notebook: AWS/DataSelection/data_global.ipynb\n",
      "  ‚Ä¢ Review Doc: REVIEW-DETR-AWS-DataSelection.md\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Status Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"DETR Dataset S3 Upload - Status Report\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ COMPLETED:\")\n",
    "print(\"  ‚Ä¢ AWS CLI configured and tested\")\n",
    "print(\"  ‚Ä¢ S3 bucket created: prahlad-siemens-hitl-demo\")\n",
    "print(\"  ‚Ä¢ DATASETS.CONSOLIDATE.json_COCO_V0() function validated\")\n",
    "print(\"  ‚Ä¢ Annotation JSONs uploaded (642.8 KiB)\")\n",
    "print(\"    - train: 713 images metadata\")\n",
    "print(\"    - valid: 134 images metadata\")\n",
    "print(\"    - test: 76 images metadata\")\n",
    "print(\"  ‚Ä¢ Total: 923 images metadata, 1,024 annotations, 3 categories\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  PENDING:\")\n",
    "print(\"  ‚Ä¢ Download 923 image files from Roboflow\")\n",
    "print(\"  ‚Ä¢ Upload images to S3 (est. 200-500 MB)\")\n",
    "\n",
    "print(\"\\nüìä S3 Structure (Current):\")\n",
    "print(\"  s3://prahlad-siemens-hitl-demo/\")\n",
    "print(\"  ‚îî‚îÄ‚îÄ raw/merged/\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ test/_annotations.coco.json\")\n",
    "print(\"      ‚îú‚îÄ‚îÄ train/_annotations.coco.json\")\n",
    "print(\"      ‚îî‚îÄ‚îÄ valid/_annotations.coco.json\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps for Team:\")\n",
    "print(\"  1. Download images from Roboflow using DETR scripts\")\n",
    "print(\"  2. Re-run this notebook to upload images\")\n",
    "print(\"  3. Implement HITL evaluation scripts\")\n",
    "print(\"  4. Build YOLOv11-OBB converter\")\n",
    "print(\"  5. Develop correction UI workflow\")\n",
    "\n",
    "print(\"\\nüìù Resources:\")\n",
    "print(f\"  ‚Ä¢ S3 Endpoint: {S3_ENDPOINT}\")\n",
    "print(f\"  ‚Ä¢ Bucket: {BUCKET_NAME}\")\n",
    "print(\"  ‚Ä¢ Notebook: AWS/DataSelection/data_global.ipynb\")\n",
    "print(\"  ‚Ä¢ Review Doc: REVIEW-DETR-AWS-DataSelection.md\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d579cc1a",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start Script (Run After Downloading ZIPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34a98aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DETR Dataset Download & Processing Automation\n",
      "======================================================================\n",
      "\n",
      "üì¶ Step 1: Checking for downloaded ZIP files...\n",
      "============================================================\n",
      "Download Roboflow COCO Dataset ZIP Files\n",
      "============================================================\n",
      "\n",
      "NOTE: These are public Roboflow datasets.\n",
      "You have two options:\n",
      "\n",
      "Option 1: Manual Download (Recommended)\n",
      "  1. Visit each dataset URL below\n",
      "  2. Click 'Download' and select 'COCO' format\n",
      "  3. Save ZIP files to workspace root or project root\n",
      "\n",
      "Option 2: Roboflow API (if you have API key)\n",
      "  Set ROBOFLOW_API_KEY environment variable\n",
      "  Or edit this script to add your API key\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  No ROBOFLOW_API_KEY found - using manual download instructions\n",
      "\n",
      "Checking for existing ZIP files...\n",
      "  ‚úó Missing: Insulators.v5i.coco.zip\n",
      "     URL: https://universe.roboflow.com/sofia-valdivieso-von-teuber/insulators-wo6lb/dataset/3\n",
      "  ‚úó Missing: song crossarm.v6i.coco.zip\n",
      "     URL: https://universe.roboflow.com/project-91iyv/song-crossarm-zqkmo/browse?queryText=&pageSize=50&startingIndex=0&browseQuery=true\n",
      "  ‚úó Missing: utility-pole.v4i.coco.zip\n",
      "     URL: https://universe.roboflow.com/project-6kpfk/utility-pole-hdbuh/browse?queryText=&pageSize=50&startingIndex=0&browseQuery=true\n",
      "\n",
      "‚ö†Ô∏è  Missing 3 ZIP file(s)\n",
      "\n",
      "To download manually:\n",
      "\n",
      "  Insulators:\n",
      "    URL: https://universe.roboflow.com/sofia-valdivieso-von-teuber/insulators-wo6lb/dataset/3\n",
      "    Expected filename: Insulators.v5i.coco.zip\n",
      "    Save to: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR or c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\n",
      "\n",
      "  Crossarm:\n",
      "    URL: https://universe.roboflow.com/project-91iyv/song-crossarm-zqkmo/browse?queryText=&pageSize=50&startingIndex=0&browseQuery=true\n",
      "    Expected filename: song crossarm.v6i.coco.zip\n",
      "    Save to: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR or c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\n",
      "\n",
      "  Utility-pole:\n",
      "    URL: https://universe.roboflow.com/project-6kpfk/utility-pole-hdbuh/browse?queryText=&pageSize=50&startingIndex=0&browseQuery=true\n",
      "    Expected filename: utility-pole.v4i.coco.zip\n",
      "    Save to: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR or c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\n",
      "\n",
      "After downloading, run:\n",
      "  python scripts/00_extract_datasets.py\n",
      "\n",
      "\n",
      "‚ö†Ô∏è  Please download the ZIP files manually first:\n",
      "\n",
      "1. Visit the Roboflow URLs listed above\n",
      "2. Download each dataset in COCO format\n",
      "3. Save ZIP files to: c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\n",
      "4. Re-run this cell\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Automated script to check, extract, and process datasets\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "DETR_PATH = Path(r\"c:\\Users\\Z0057P7S\\OneDrive - Siemens Energy\\Documents\\MenonSiemens\\DETR\\utility-inventory-detr-main\")\n",
    "SCRIPTS_PATH = DETR_PATH / \"scripts\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DETR Dataset Download & Processing Automation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Check for ZIP files\n",
    "print(\"\\nüì¶ Step 1: Checking for downloaded ZIP files...\")\n",
    "result = subprocess.run(\n",
    "    [sys.executable, str(SCRIPTS_PATH / \"01_download_datasets.py\")],\n",
    "    cwd=str(DETR_PATH),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "\n",
    "if \"All ZIP files already exist\" in result.stdout:\n",
    "    print(\"‚úÖ All ZIPs found! Proceeding to extraction...\\n\")\n",
    "    \n",
    "    # Step 2: Extract\n",
    "    print(\"üìÇ Step 2: Extracting ZIP files...\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(SCRIPTS_PATH / \"00_extract_datasets.py\")],\n",
    "        cwd=str(DETR_PATH),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    \n",
    "    # Step 3: Clean\n",
    "    print(\"\\nüßπ Step 3: Cleaning datasets...\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(SCRIPTS_PATH / \"02_clean_datasets.py\")],\n",
    "        cwd=str(DETR_PATH),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    \n",
    "    # Step 4: Merge\n",
    "    print(\"\\nüîÄ Step 4: Merging datasets...\")\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(SCRIPTS_PATH / \"03_merge_datasets.py\")],\n",
    "        cwd=str(DETR_PATH),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ DATASET PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüì§ Next: Re-run the S3 upload cells above to upload images\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Please download the ZIP files manually first:\")\n",
    "    print(\"\\n1. Visit the Roboflow URLs listed above\")\n",
    "    print(\"2. Download each dataset in COCO format\")\n",
    "    print(f\"3. Save ZIP files to: {DETR_PATH}\")\n",
    "    print(\"4. Re-run this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
